{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bc430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as klayers\n",
    "import numpy as np, math\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "import gradnet\n",
    "from gradnet import layers as glayers\n",
    "from gradnet.activations import get_activation\n",
    "from gradnet.losses import get_loss, Loss\n",
    "from gradnet.optimizers import get_optimizer\n",
    "\n",
    "nx = 3\n",
    "hidden = 5\n",
    "na = 2\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyLoss(Loss):\n",
    "    \n",
    "    def compute(self, data):\n",
    "        probs = self.Inputs[0].Y\n",
    "        n = probs.shape[-1]\n",
    "        nlog = math.log(n)\n",
    "        values = -np.sum(probs*np.log(probs), axis=-1)\n",
    "        grads = (np.log(probs)+1.0)\n",
    "        self.Grads = [grads]\n",
    "        self.Values = values        \n",
    "        return self.value\n",
    "\n",
    "class ActorLoss(Loss):\n",
    "    \n",
    "    def compute(self, data):\n",
    "        returns = data[\"returns\"]\n",
    "        actions = data[\"actions\"]\n",
    "        probs, values = self.Inputs[0].Y, self.Inputs[1].Y\n",
    "        values = values[:,0]\n",
    "        #print(\"ActorLoss: probs:\",probs)\n",
    "        #print(\"          values:\", values)\n",
    "        action_mask = np.eye(probs.shape[-1])[actions]\n",
    "        action_probs = np.sum(action_mask*probs, axis=-1)\n",
    "        advantages = returns - values\n",
    "        #print(\"ActorLoss: rewards:\", rewards.shape, \"   values:\", values.shape, \"   probs:\", probs.shape, \"   advantages:\", advantages.shape,\n",
    "        #    \"   action_probs:\", action_probs.shape)\n",
    "        loss_grads = -advantages/np.clip(action_probs, 1.e-5, None)  \n",
    "        grads = action_mask * loss_grads[:,None]\n",
    "        if True:\n",
    "            print(\"ActorLoss:\")\n",
    "            #print(\"      probs:\", probs)\n",
    "            print(\"    actions:\", actions)\n",
    "            print(\"   probs[a]:\", action_probs)\n",
    "            print(\"    returns:\", returns)\n",
    "            print(\"     values:\", values)\n",
    "            print(\"  advatages:\", advantages)\n",
    "            #print(\"action_mask:\", action_mask)\n",
    "            print(\" loss_grads:\", loss_grads)\n",
    "            print(\"      grads:\", grads)\n",
    "            print(\"---------------------\")\n",
    "        \n",
    "        self.Grads = [grads, None]\n",
    "        self.Values = loss_grads        # not really loss values\n",
    "        return self.value\n",
    "\n",
    "class InvalidActionLoss(Loss):\n",
    "    \n",
    "    def compute(self, data):\n",
    "        probs = self.Inputs[0].Y\n",
    "        valid_mask = data.get(\"valids\")\n",
    "        if valid_mask is not None:\n",
    "            self.Values = np.mean(probs*probs*(1-valid_mask), axis=-1)\n",
    "            self.Grads = [2*(1-valid_mask)*probs]\n",
    "        else:\n",
    "            self.Values = np.zeros((len(probs),))\n",
    "            self.Grads = [None]\n",
    "        return self.value\n",
    "\n",
    "\n",
    "def create_gradnet_model(nx, num_actions, hidden):\n",
    "        inp = gradnet.Input((nx,), name=\"input\")\n",
    "        common1 = glayers.Dense(hidden, activation=\"tanh\", name=\"common1\")(inp)\n",
    "        common = glayers.Dense(hidden//2, activation=\"tanh\", name=\"common\")(common1)\n",
    "\n",
    "        #action1 = Dense(max(hidden//5, num_actions//2), activation=\"relu\", name=\"action1\")(common)\n",
    "        probs = glayers.Dense(num_actions, name=\"action\", activation=\"softmax\")(common)\n",
    "        \n",
    "        #critic1 = Dense(hidden//5, name=\"critic1\", activation=\"relu\")(common)\n",
    "        value = glayers.Dense(1, name=\"critic\")(common)\n",
    "\n",
    "        model = gradnet.Model([inp], [probs, value])\n",
    "        \n",
    "        model.add_loss(ActorLoss(probs, value), 1.0, name=\"actor_loss\")\n",
    "        model.add_loss(get_loss(\"mse\")(value), name=\"critic_loss\")\n",
    "        model.add_loss(EntropyLoss(probs), 1.0, name=\"entropy_loss\")\n",
    "        #model.add_loss(InvalidActionLoss(probs), 0.0, name=\"invalid_action_loss\")\n",
    "        model.compile(optimizer=get_optimizer(\"sgd\", learning_rate=learning_rate))\n",
    "        return model\n",
    "\n",
    "def create_keras_model(nx, num_actions, hidden):\n",
    "        inp = keras.Input((nx,), name=\"input\")\n",
    "        common1 = klayers.Dense(hidden, activation=\"tanh\", name=\"common1\")(inp)\n",
    "        common = klayers.Dense(hidden//2, activation=\"tanh\", name=\"common\")(common1)\n",
    "\n",
    "        #action1 = Dense(max(hidden//5, num_actions//2), activation=\"relu\", name=\"action1\")(common)\n",
    "        probs = klayers.Dense(num_actions, name=\"action\", activation=\"softmax\")(common)\n",
    "        \n",
    "        #critic1 = Dense(hidden//5, name=\"critic1\", activation=\"relu\")(common)\n",
    "        value = klayers.Dense(1, name=\"critic\")(common)\n",
    "\n",
    "        model = keras.Model(inp, [probs, value])\n",
    "        return model\n",
    "    \n",
    "def ggrads(observations, actions, action_mask, returns):\n",
    "    #\n",
    "    # gmodel grads\n",
    "    #\n",
    "\n",
    "    gmodel.reset_losses()\n",
    "    probs, values = gmodel.call(observations)\n",
    "    #print(\"probs:\", probs)\n",
    "    #print(\"values:\", values)\n",
    "    advantages = returns - values\n",
    "    gmodel.backprop(returns[:,None], dict(\n",
    "        actions = actions,\n",
    "        returns = returns,\n",
    "        valids = None\n",
    "    ))\n",
    "    for l in gmodel.layers:\n",
    "        lg = l.PGradSum\n",
    "        if lg:\n",
    "            pass\n",
    "            #print(\"layer:\", l)\n",
    "            #for g in lg:\n",
    "            #    print(\"-\", g)\n",
    "    return gmodel.layer_gradients()\n",
    "\n",
    "def kgrads(observations, actions, action_mask, returns):\n",
    "    log_n_actions = math.log(na)\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "            all_losses = []\n",
    "\n",
    "            probs, values = kmodel(observations)\n",
    "            values = values[:,0]\n",
    "\n",
    "            advantages = returns - values\n",
    "            episode_critic_loss = tf.reduce_sum(advantages*advantages)\n",
    "            all_losses.append(episode_critic_loss)\n",
    "\n",
    "            action_probs = tf.reduce_sum(probs*action_mask, axis=-1)\n",
    "            logprobs = tf.math.log(tf.clip_by_value(action_probs, 1e-5, 1-1e-5))\n",
    "            problosses = -logprobs * advantages.numpy()\n",
    "            episode_actor_loss = tf.reduce_sum(problosses)\n",
    "            all_losses.append(episode_actor_loss)\n",
    "\n",
    "            \n",
    "            entropy_per_step = -tf.reduce_sum(probs*tf.math.log(tf.clip_by_value(probs, 1e-5, 1.0)), axis=-1)            \n",
    "            episode_entropy_loss = -tf.reduce_sum(entropy_per_step)\n",
    "            all_losses.append(episode_entropy_loss)\n",
    "            \n",
    "            total_loss = sum(all_losses)\n",
    "            print(\"total loss:\", total_loss.numpy())\n",
    "            grads = tape.gradient(total_loss, kmodel.trainable_weights)\n",
    "    return [g.numpy() if g is not None else None for g in grads]\n",
    "\n",
    "\n",
    "np.random.seed(32)\n",
    "gmodel = create_gradnet_model(nx, na, hidden)\n",
    "kmodel = create_keras_model(nx, na, hidden)\n",
    "\n",
    "kweights = {}\n",
    "\n",
    "for l in kmodel.layers:\n",
    "    wlst = l.get_weights()\n",
    "    if wlst:\n",
    "        #print(l.name)\n",
    "        #for w in wlst:\n",
    "        #    print(\"   \", w)\n",
    "        kweights[l.name] = wlst\n",
    "\n",
    "for l in gmodel.layers:\n",
    "    #print(type(l), l)\n",
    "    name = l.Name\n",
    "    if name:\n",
    "        w = kweights[name]\n",
    "        #print(\"keras weight for\", name,\":\", w)\n",
    "        l.set_weights(w)\n",
    "        #print(name, \"weights:\", l.get_weights())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab122f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.720675  0.914968 -0.54842 ]\n",
      " [-1.720675  0.914968 -0.54842 ]]\n",
      "kmodel out: [<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[0.424954, 0.575046],\n",
      "       [0.424954, 0.575046]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[-0.232155],\n",
      "       [-0.232155]], dtype=float32)>]\n",
      "gmodel out: [array([[0.424954, 0.575046],\n",
      "       [0.424954, 0.575046]]), array([[-0.232155],\n",
      "       [-0.232155]])]\n"
     ]
    }
   ],
   "source": [
    "mb = 2\n",
    "\n",
    "np.random.seed(55)\n",
    "obs = list(np.random.random((nx,))*3-2)\n",
    "\n",
    "actions = np.array([0]*mb)\n",
    "action_mask = np.zeros((mb, na))\n",
    "action_mask[:,0] = 1.0\n",
    "rewards = np.array([0.5]*mb)\n",
    "returns = np.array([0.5]*mb)\n",
    "observations = np.array([obs]*mb)\n",
    "\n",
    "print(observations)\n",
    "\n",
    "print(\"kmodel out:\", kmodel(observations))\n",
    "print(\"gmodel out:\", gmodel.call(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a44a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads:\n",
      "ActorLoss:\n",
      "    actions: [0 0]\n",
      "   probs[a]: [0.424954 0.424954]\n",
      "    returns: [0.5 0.5]\n",
      "     values: [-0.232155 -0.232155]\n",
      "  advatages: [0.732155 0.732155]\n",
      " loss_grads: [-1.722904 -1.722904]\n",
      "      grads: [[-1.722904 -0.      ]\n",
      " [-1.722904 -0.      ]]\n",
      "---------------------\n",
      "g: [[-0.260118  0.260118]\n",
      " [-0.373688  0.373688]]\n",
      "g: [-0.989873  0.989873]\n",
      "g: [[ 1.197864 -2.685358]\n",
      " [ 0.640267 -1.435342]\n",
      " [ 0.136491 -0.305985]\n",
      " [ 0.158718 -0.355812]\n",
      " [ 1.325648 -2.971823]]\n",
      "g: [-1.389702  3.115417]\n",
      "g: [[ 0.48585  -0.832593  2.174002  5.616465 -0.049317]\n",
      " [-0.258351  0.442731 -1.156024 -2.986551  0.026224]\n",
      " [ 0.154852 -0.265367  0.692906  1.790101 -0.015719]]\n",
      "g: [-0.28236   0.483876 -1.263459 -3.264106  0.028661]\n",
      "g: [[-0.769579]\n",
      " [-1.105585]]\n",
      "g: [-2.92862]\n",
      "delta: [[ 0.002601 -0.002601]\n",
      " [ 0.003737 -0.003737]]\n",
      "delta: [ 0.009899 -0.009899]\n",
      "delta: [[-0.011979  0.026854]\n",
      " [-0.006403  0.014353]\n",
      " [-0.001365  0.00306 ]\n",
      " [-0.001587  0.003558]\n",
      " [-0.013256  0.029718]]\n",
      "delta: [ 0.013897 -0.031154]\n",
      "delta: [[-0.004859  0.008326 -0.02174  -0.056165  0.000493]\n",
      " [ 0.002584 -0.004427  0.01156   0.029866 -0.000262]\n",
      " [-0.001549  0.002654 -0.006929 -0.017901  0.000157]]\n",
      "delta: [ 0.002824 -0.004839  0.012635  0.032641 -0.000287]\n",
      "delta: [[0.007696]\n",
      " [0.011056]]\n",
      "delta: [0.029286]\n"
     ]
    }
   ],
   "source": [
    "print(\"grads:\")\n",
    "for g in ggrads(observations, actions, action_mask, returns):\n",
    "    print(\"g:\", g)\n",
    "\n",
    "deltas = gmodel.apply_deltas()\n",
    "for lst in deltas:\n",
    "    for d in lst:\n",
    "        print(\"delta:\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e9456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads:\n",
      "total loss: 0.96153975\n",
      "g: [[ 0.48585  -0.832593  2.174002  5.616465 -0.049317]\n",
      " [-0.258351  0.442731 -1.156024 -2.986551  0.026224]\n",
      " [ 0.154852 -0.265367  0.692906  1.790101 -0.015719]]\n",
      "g: [-0.28236   0.483876 -1.263459 -3.264106  0.028661]\n",
      "g: [[ 1.197864 -2.685358]\n",
      " [ 0.640267 -1.435342]\n",
      " [ 0.136491 -0.305985]\n",
      " [ 0.158718 -0.355812]\n",
      " [ 1.325648 -2.971822]]\n",
      "g: [-1.389702  3.115417]\n",
      "g: [[-0.260118  0.260118]\n",
      " [-0.373688  0.373688]]\n",
      "g: [-0.989873  0.989873]\n",
      "g: [[-0.769579]\n",
      " [-1.105585]]\n",
      "g: [-2.92862]\n",
      "delta: [[-0.004858  0.008326 -0.02174  -0.056165  0.000493]\n",
      " [ 0.002584 -0.004427  0.01156   0.029866 -0.000262]\n",
      " [-0.001549  0.002654 -0.006929 -0.017901  0.000157]]\n",
      "delta: [ 0.002824 -0.004839  0.012635  0.032641 -0.000287]\n",
      "delta: [[-0.011979  0.026854]\n",
      " [-0.006403  0.014353]\n",
      " [-0.001365  0.00306 ]\n",
      " [-0.001587  0.003558]\n",
      " [-0.013256  0.029718]]\n",
      "delta: [ 0.013897 -0.031154]\n",
      "delta: [[ 0.002601 -0.002601]\n",
      " [ 0.003737 -0.003737]]\n",
      "delta: [ 0.009899 -0.009899]\n",
      "delta: [[0.007696]\n",
      " [0.011056]]\n",
      "delta: [0.029286]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# keras grads\n",
    "#\n",
    "\n",
    "\n",
    "print(\"grads:\")\n",
    "kgs = kgrads(observations, actions, action_mask, returns)\n",
    "for kg in kgs:\n",
    "    print(\"g:\", kg)\n",
    "    \n",
    "old_weights = kmodel.get_weights()\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "optimizer.apply_gradients(zip(kgs, kmodel.trainable_variables))\n",
    "\n",
    "deltas = [p1-p0 for p0, p1 in zip(old_weights, kmodel.get_weights())]\n",
    "\n",
    "for d in deltas:\n",
    "    print(\"delta:\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46983be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    x = np.random.random((5,))\n",
    "    t = np.tanh(x)\n",
    "    d = 1-t**2\n",
    "    print(x, t, d)\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    tape.watch(x)\n",
    "    y = tf.reduce_sum(tf.math.tanh(x))\n",
    "    print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "x = np.array([[0.1, 0.2]])\n",
    "#x = np.random.random((1,3))\n",
    "\n",
    "i = keras.Input((2,))\n",
    "d = klayers.Dense(1, activation=\"tanh\", kernel_initializer=keras.initializers.Ones(), name=\"dense\")(i)\n",
    "kmodel = keras.Model(i, d)\n",
    "\n",
    "inp = gradnet.Input((2,))\n",
    "dd = glayers.Dense(1, activation=\"tanh\", name=\"dense\")(inp)\n",
    "gmodel = gradnet.Model(inp, dd)\n",
    "gmodel.add_loss(get_loss(\"mse\")(dd), name=\"loss\")\n",
    "\n",
    "kweights = {}\n",
    "\n",
    "for l in kmodel.layers:\n",
    "    wlst = l.get_weights()\n",
    "    if wlst:\n",
    "        print(l.name)\n",
    "        for w in wlst:\n",
    "            print(\"   \", w)\n",
    "        kweights[l.name] = wlst\n",
    "\n",
    "for l in gmodel.layers:\n",
    "    #print(type(l), l)\n",
    "    name = l.Name\n",
    "    if name:\n",
    "        w = kweights[name]\n",
    "        #print(\"keras weight for\", name,\":\", w)\n",
    "        l.set_weights(w)\n",
    "        #print(name, \"weights:\", l.get_weights())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    xt = tf.convert_to_tensor(x)\n",
    "    tape.watch(xt)\n",
    "    y = kmodel(xt)\n",
    "    L = tf.reduce_sum(y*y)\n",
    "    grads = tape.gradient(L, xt)\n",
    "    print(\"keras y:\", y.numpy(), \"     L:\", L.numpy(), \"   tf grads:\", grads)\n",
    "    \n",
    "\n",
    "gmodel.reset_losses()\n",
    "y = gmodel.call(x)[0]\n",
    "L = gmodel.backprop(np.zeros_like(y))[\"loss\"]\n",
    "print(\"gradnet y:\", y, \"   L:\", L)\n",
    "print(\"gradnet grads:\", gmodel.input_gradients())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39468b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = np.zeros((1,1))\n",
    "x0 = x.copy()\n",
    "x1 = x.copy()\n",
    "dx = 0.001\n",
    "x1[0,0] += dx\n",
    "\n",
    "gmodel.call(x0)\n",
    "v1 = gmodel.backprop(y_)[\"loss\"]\n",
    "gmodel.call(x1)\n",
    "v2 = gmodel.backprop(y_)[\"loss\"]\n",
    "\n",
    "print(v1, v2, (v2-v1)/dx)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ab61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
